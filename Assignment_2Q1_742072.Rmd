---
title: 'Assignment 2, Question 1 MAST90125: Bayesian Statistical Learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 20 September 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(742072)  #Please change random seed to your student id number.
```

## Question One (12 marks)

In generalised linear models, rather than estimating effects from the response data directly, we model through a link function, $\eta(\bm \theta)$, and assume $\eta(\bm \theta)_i = {\bf x}_i'\bm \beta$. The link function can be determined by re-arranging the likelihood of interest into the exponential family format,  	
\begin{eqnarray} p(y|\bm \theta) = f(y)g(\bm \theta)e^{\eta(\bm \theta)'u(y)}. \end{eqnarray} 

a) Re-arrange the Poisson probability mass function into the exponential family format to determine the canonical link function. The Poisson pmf is
\[ Pr(y|\lambda) = \frac{\lambda^ye^{-\lambda}}{y!}.  \]

The Poisson likelihood can be re-arranged into form:
\begin{align*}
  \Pr(y|\lambda)&= \frac{1}{y!}e^{-\lambda}\lambda^y\\
  &= \frac{1}{y!}e^{-\lambda} \exp{y\log\lambda}\\
  f(y)&= \frac{1}{y!},g(\lambda)=e^{-\lambda},\eta(\lambda)= \log{\lambda},u(y)=y.
\end{align*}

To explore some properties of Metropolis sampling, consider the dataset \texttt{Warpbreaks.csv}, which is on LMS. This dataset contains information of the number of breaks in a consignment of wool. In addition, Wool type (A or B) and tension level (L, M or H) was recorded. 

b) Fit a Poisson regression to the warpbreak data, with Wool type and tension treated as factors using the function \text{glm} in R. Report co-efficient estimates and the variance-covariance matrix.

```{r}
#read data
Warpbreaks<-read.csv("Warpbreaks.csv")
#the mean value is
mean(Warpbreaks$breaks)
#the variance is
var(Warpbreaks$breaks) #calculate variance
#poisson regression with glm 
poisson.model<-glm(breaks~wool+tension, Warpbreaks, family = poisson(link = "log"))
summary(poisson.model)
library(arm)
#extract coefficient
coeff=coef(poisson.model)
#extract standard error
sterr=se.coef(poisson.model)
```
The coefficient estimates are
```{r}
coeff
```
and the covariance matrix is
```{r}
#covariance matrix
vcov(poisson.model)
```


c) Fit a Bayesian Poisson regression using Metropolis sampling. Assume flat priors for all coefficients. Extract the design matrix $\bf X$ from the \texttt{glm} fitted in a). For the proposal distribution, use a Normal distribution with mean $\theta^{t-1}$ and variance-covariance matrix $c^2\hat{\bm \Sigma}$ where ${\bm \Sigma}$ is the variance-covariance matrix from the glm fit. Consider three candidates for $c$, $1.6/\sqrt{p}, 2.4/\sqrt{p}, 3.2/\sqrt{p}$, where $p$ is the number of parameters estimated. Run the Metropolis algorithm for 10,000 iterations, and discard the first 5,000. Report the following:

\begin{itemize}
\item Check, using graphs and appropriate statistics, that each chain converges to the same distribution. To do this, you may find installing the R package \texttt{coda} helpful.
\item The proportion of candidate draws that were accepted.
\item The effective sample size for each chain. 
\item What do you think is the best choice for $c$. Does this match the results stated in class on efficiency and optimal acceptance rate?
\end{itemize}

```{r}
#Function for performing Metropolis sampling for Poisson regression
#Inputs:
#y: vector of responses
#n: vector (or scalar) of trial sizes. 
#X: predictor matrix including intercept.
#c: rescaling for variance-covariance matrix, scalar J(theta*|theta(t-1)) = N(theta(t-1), c^2*Sigma)
#Sigma is variance covariance matrix for parameters in J()
#iter: number of iterations
#burnin: number of initial iterations to throw out.
Metropolis.poi<-function(y,n,X,c,Sigma,iter,burnin,init){ 
#number of parameters
p<-dim(X)[2]   
library(mvtnorm)
#initialize values from normal distribution
theta0<-init 
#construct (iterations by parameters in dimension)-matrix to store iterations 
theta_sim<-matrix(0,iter,p)
#apply array of initial value 
theta_sim[1,]<-theta0
count_accept=0
#fill in the rest of matrix
for(i in 1:(iter-1)){
#draw candidate (jointly),according to mean=previous array of value and determined covariance matrix
theta_cand <-rmvnorm(1,mean=theta_sim[i,],sigma=(c^2)*Sigma) 
#extract the numerical value of candidate draws
theta_cand <-as.numeric(theta_cand)  
#candidate matrix product X'beta 
xb_cand <-X%*%theta_cand 
#Calculating mean for candidates.
lambda_cand<-exp(xb_cand)   
#current product 
xb<-X%*%theta_sim[i,]
#Calculating mean for theta(t-1). 
lambda_current<-exp(xb)    
#difference of log joint distributions.
r<-sum(dpois(y,lambda=lambda_cand,log=TRUE)-dpois(y,lambda=lambda_current,log=TRUE))
#Draw an indicator whether to accept/reject candidate
ind<-rbinom(1,1,exp( min(c(r,0)) ) )
theta_sim[i+1,]<- ind*theta_cand + (1-ind)*theta_sim[i,]
#record the accepted draws
count_accept<-count_accept+ind
}

#accepted draws ratio
accept_draws<-count_accept/iter
results <- list()
  #results$first <- theta_sim[(burnin+1):iter,]
   results$first <- theta_sim[(burnin+1):iter,]
  results$second <-accept_draws
  return(results) 
}
```
Now, we import the data and variables we would like to use in the function of Metropolis algorithm: 
```{r}
#formatting data into the correct format. 
#Build predictor matrix.
pred.mat <- model.matrix(breaks ~ wool + tension, Warpbreaks)
#build response
Y<-Warpbreaks$breaks
#build covariance matrix
sigma<-vcov(poisson.model)
#number of parameter to estimate
p<-4
#Metropolis algorithm for three candidates of c
iterations<-10000
burnin<-5000
```

We choose to one chain per c value, each chain starts from an initial value that is randomly selected from the same normal distribution.

Generate 3 chains:
```{r}
c1_chain1<-Metropolis.poi(y=Y,n=10,X=pred.mat,c=1.6/sqrt(p),Sigma=sigma,iter=10000,burnin=5000,init=c(1,2,3,4))
c2_chain1<-Metropolis.poi(y=Y,n=10,X=pred.mat,c=2.4/sqrt(p),Sigma=sigma,iter=10000,burnin=5000,init=c(1,2,3,4))
c3_chain1<-Metropolis.poi(y=Y,n=10,X=pred.mat,c=3.2/sqrt(p),Sigma=sigma,iter=10000,burnin=5000,init=rnorm(p))
```


Now we examine the convergence of chains under different c values
```{r}
#extract chains for each parameter from each chain
b0c1<-as.mcmc(c1_chain1$first[,1])
b1c1<-as.mcmc(c1_chain1$first[,2])
b2c1<-as.mcmc(c1_chain1$first[,3])
b3c1<-as.mcmc(c1_chain1$first[,4])

b0c2<-as.mcmc(c2_chain1$first[,1])
b1c2<-as.mcmc(c2_chain1$first[,2])
b2c2<-as.mcmc(c2_chain1$first[,3])
b3c2<-as.mcmc(c2_chain1$first[,4])

b0c3<-as.mcmc(c3_chain1$first[,1])
b1c3<-as.mcmc(c3_chain1$first[,2])
b2c3<-as.mcmc(c3_chain1$first[,3])
b3c3<-as.mcmc(c3_chain1$first[,4])

#combine the chains for each parameter, under different c value
b0chains<-mcmc.list(b0c1,b0c2,b0c3)
b1chains<-mcmc.list(b1c1,b1c2,b1c3)
b2chains<-mcmc.list(b2c1,b2c2,b2c3)
b3chains<-mcmc.list(b3c1,b3c2,b3c3)


coda::traceplot(b0chains,smooth=TRUE)
coda::traceplot(b1chains,smooth=TRUE)
coda::traceplot(b2chains,smooth=TRUE)
coda::traceplot(b3chains,smooth=TRUE)
#the convergence plot of chains and density

plot(density(b0c1),col=1)
lines(density(b0c2),col=2)
lines(density(b0c3),col=3)


plot(density(b1c1),col=1)
lines(density(b1c2),col=2)
lines(density(b1c3),col=3)

plot(density(b2c1),col=1)
lines(density(b2c2),col=2)
lines(density(b2c3),col=3)

plot(density(b3c1),col=1)
lines(density(b3c2),col=2)
lines(density(b3c3),col=3)

#Perform Gelman diagonosis,parameter-wise,for three chains
gelman.diag(b0chains,confidence=0.95,transform=FALSE)
gelman.diag(b1chains,confidence=0.95,transform=FALSE)
gelman.diag(b2chains,confidence=0.95,transform=FALSE)
gelman.diag(b3chains,confidence=0.95,transform=FALSE)

gelman.plot(b0chains,confidence=0.95,transform=FALSE)
gelman.plot(b1chains,confidence=0.95,transform=FALSE)
gelman.plot(b2chains,confidence=0.95,transform=FALSE)
gelman.plot(b3chains,confidence=0.95,transform=FALSE)

#the effective sample sizes of three chains
eb0<-lapply(b0chains,effectiveSize)
eb1<-lapply(b1chains,effectiveSize)
eb2<-lapply(b2chains,effectiveSize)
eb3<-lapply(b3chains,effectiveSize)

#plot autocorrelation
autob0<-acfplot(b0chains,aspect="x")
autob1<-acfplot(b1chains,aspect="x")
autob2<-acfplot(b2chains,aspect="x")
autob3<-acfplot(b3chains,aspect="x")

```

2. accepted draws
$c=1.6/\sqrt{p}$:
```{r}
c1_chain1$second
```
$c=2.4/\sqrt{p}$:
```{r}
c2_chain1$second
```
$c=3.2/\sqrt{p}$:
```{r}
c3_chain1$second
```

3. effective sample size
for all coefficients, the effective sample sizes for three $c$ values are: 

$c=1.6/\sqrt{p}$:
```{r}
c(as.numeric(eb0)+as.numeric(eb1)+as.numeric(eb2)+as.numeric(eb3))[1]
```
$c=2.4/\sqrt{p}$:
```{r}
c(as.numeric(eb0)+as.numeric(eb1)+as.numeric(eb2)+as.numeric(eb3))[2]
```
$c=3.2/\sqrt{p}$:
```{r}
c(as.numeric(eb0)+as.numeric(eb1)+as.numeric(eb2)+as.numeric(eb3))[1]
```

When $c=2.4/\sqrt{p}$, the effective sample size is maximum. It says that the autocorrelation under this $c$ value the autocorrelation is minimum.


