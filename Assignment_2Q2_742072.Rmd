---
title: 'Assignment 2, Question 2 MAST90125: Bayesian Statistical Learning'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Due: Friday 20 September 2019**  
\vspace{5 mm}

**There are places in this assignment where R code will be required. Therefore set the random seed so assignment is reproducible.**

```{r}
set.seed(742072)  #Please change random seed to your student id number.
```

## Question Two (20 marks) 

In lecture 3, we discussed how a Bayesian framework readily lends itself to combining information from sequential experiments. To demonstrate, consider the following data extracted from the *HealthIron* study. 


Serum ferritin levels were measured for two samples of women, one of C282Y homozygotes $(n = 88)$ and the other of women with neither of the key mutations (C282Y and H63D) in the HFE gene, so-called HFE \lq wildtypes\rq $(n= 242)$. The information available is 

\begin{itemize}
\item \texttt{idnum}: Participant id.
\item \texttt{homc282y}: Indicator whether individual is Homozygote (1) or Wildtype (0).
\item  \texttt{time}: Time since onset of menopause, measured in years.
\item \texttt{logsf}: The natural logarithm of the serum ferritin in $\mu$g/L.
\end{itemize}


The data required to answer this question are \texttt{Hiron.csv}, which can be downloaded from LMS.

a) Fit a standard linear regression,

\[ E(\text{logsf}) = \beta_0 + \beta_1\text{time} \]

with responses restricted to those who are homozygote (\texttt{homc282y} = 1). This can be done using the \texttt{lm} function in R. Report the estimated coefficients $\hat{\bm \beta}$, estimated error variance, $\hat{\sigma}^2_e$ and $({\bf X}'{\bf X})^{-1}$.

Perform linear regression
```{r}
hiron<-read.csv("Hiron.csv")
hiron_1<-hiron[hiron$homc282y==1,]
hiron_2<-hiron[hiron$homc282y==0,]
linear_model<-lm(logsf~time, data=hiron_1)
linear_model_2<-lm(logsf~time, data=hiron_2)
summary(linear_model)
#sum of residuals and calculate the error variance
SSE<-(1/(length(linear_model$residuals)-2))*sum((linear_model$residuals)^2)

#xTxinv<-1/((hiron_1$time)%*%(hiron_1$time))
#XTXinv<-Solve(..,..)
```
The estimation shows 
$\hat{\beta}_0 = 4.2398$ with standard deviation $0.24053$; $\hat{\beta}_1=0.0713$ with standard deviation $0.01350$. 

The estimated error variance $\hat{\sigma}_{e}^2$ is 
```{r}
SSE
```
And the matrix $(\textbf{X}^T \textbf{X})^{-1}$ is 
```{r}
xTxinv
```

b) Fit a Bayesian regression using a Gibbs sampler to **only the wildtype (\texttt{homc282y}=0) data**. Use the output from your answer in a) to define proper priors for $\bm \beta, \bm \tau$. For help, refer to lecture 13.  For the Gibbs sampler, run two chains for 10,000 iterations.  Discard the first 1000 iterations as burn-in and then remove every second remaining iteration to reduce auto-correlation. When storing results, convert $\tau$ back to $\sigma^2$.  When running the Gibbs sampler, incorporate posterior predictive checking, using the test statistic $T(y,\bm \beta) =\sum_{i=1}^n e_i^2$ and $T(y^\text{rep},\bm \beta) =\sum_{i=1}^n ({e^\text{rep}_i})^2$, where $e_i$ is the predicted residual for observation $i$ at simulation $j$ and $e^\text{rep}_i$ is the replicate residual for observation $i$ at simulation $j$. Report posterior means, standard deviations and 95 \% central credible intervals for $\beta_0, \beta_1, \sigma^2$ combining results for the two chains

We use unblocked Gibbs sampler. To speed up the computation, we choose the method of singular value decomposition to decompose the matrix $X$, sample the values and transform to $\beta$.

First, build up the function of Gibbs Sampler.

```{r}

Gibbs.lmsvd<-function(X_prev,y_prev,X,y,tau0,iter,burnin){
count<-replicate(iter,0)
p <- dim(X)[2]
#so everything has to be dependent on previous dataset
K<-solve(t(X_prev)%*%X_prev) #K=invXTX
n_1<-dim(X_prev)[1]
n_2<-dim(X)[1]
a<-t(X)%*%y+t(X_prev)%*%y_prev

#variance-covariance matrix
invK<-t(X_prev)%*%X_prev
Sigmainv<-t(X)%*%X+invK
vcov<-solve(Sigmainv)  #variance covariance matrix
mu_beta<-vcov%*%a #known

#matrix decomposition
svdSigma <-svd(Sigmainv)   #matrix decomposition to speed up computation.
U    <-svdSigma$u
Lambda<-svdSigma$d #eigenvalue matrix
V    <-svdSigma$v 
Vbetamu<-t(V)%*%mu_beta

#initialize value
beta_0<-K%*%(t(X_prev)%*%y_prev)
tau <-tau0
vbeta<-rnorm(p)
par<-matrix(0,iter,p+1)  
count_test <-0
betahat<-K%*%(t(X_prev)%*%y_prev)
s_squared_prev<-sum((y_prev-(X_prev%*%betahat))^2) #previous sum of squares

for( i in 1:iter){
  #sample beta
  vbeta <- rnorm(p,mean=Vbetamu,sqrt(1/(tau*Lambda)))
  #transform beta
  beta <-V%*%vbeta   
  s_squared <-sum((y-(X%*%beta))^2) #new sum of squares
  #sample tau
  tau <- rgamma(1,(1/2)*(n_1+n_2),(1/2)*s_squared_prev+(1/2)*s_squared+(1/2)*(t(beta-beta_0)%*%(invK%*%(beta-beta_0))))
  sigma_e<-1/tau
  par[i,] <-c(beta,sigma_e)
  
  #posterior predictive checking
  t_y<-s_squared
  
  mean_rep<-X%*%beta
  y_rep<-rnorm(n_2,mean=mean_rep,1/sqrt(tau))
  t_rep<-sum((y_rep-mean_rep)^2)
  
  if(t_rep>=t_y){count[i]<-1}
}
count<-count[(burnin+1):iter]
par <-par[(burnin+1):iter,] 
results<-c()
results$first<-count
results$second<-par
return(results)  
}
```


```{r}
iterations<-10000
burn<-5000
cut<-2500
#Formatting data, and running chains.
hiron<-read.csv("Hiron.csv")
hiron_0<-hiron[hiron$homc282y==0,]  #response variable
hiron_1<-hiron[hiron$homc282y==1,]
n<-dim(hiron)[1]
intercept <-matrix(1,n,1) #Intercept (to be estimated without penalty)
y<-as.numeric(hiron_0$logsf)  #response
Pred<-as.numeric(hiron_0$time)       #preditor
X<-cbind(intercept[1:dim(hiron_0)[1]],Pred)

response_prev<-as.numeric(hiron_1$logsf)
Pred_prev<-hiron_1$time

X_prev<-cbind(intercept[dim(hiron_1)[1]],Pred_prev)
#perform sampling
run1<-Gibbs.lmsvd(X_prev=X_prev,y_prev=response_prev,X=X,y=y,tau0=1,iter=iterations,burnin=burn)
count1<-run1$first
chain1<-run1$second

run2<-Gibbs.lmsvd(X_prev=X_prev,y_prev=response_prev,X=X,y=y,tau0=5,iter=iterations,burnin=burn)
count2<-run2$first
chain2<-run2$second

library(coda)
ml1a<-as.mcmc.list(as.mcmc((chain1[1:cut,])))
ml1b<-as.mcmc.list(as.mcmc((chain1[(cut+1):burn,])))
ml2a<-as.mcmc.list(as.mcmc((chain2[1:cut,])))
ml2b<-as.mcmc.list(as.mcmc((chain2[(cut+1):burn,])))
estml<-c(ml1a,ml1b,ml2a,ml2b)
#Gelman-Rubin diagnostic.

gelman.diag(estml)[[1]]
gelman.diag(c(as.mcmc.list(as.mcmc(chain1)),as.mcmc.list(as.mcmc(chain2))))
plot(mcmc.list(estml))
#effective sample size.
 
effectiveSize(estml)
gelman.plot(estml)[[1]]
#Reporting posterior means and credible intervals.
#Means
colMeans(rbind(chain1,chain2)) 
#95 % central Credible interval
apply(rbind(chain1,chain2) ,2, FUN =function(x) quantile(x,c(0.025,0.975) )) 
#
apply(rbind(chain1,chain2) ,2, FUN =function(x) sd(x) )

#check ratio
sum(count1)/5000
sum(count2)/5000
```


c) Perform convergence checks for the chain obtained in b). Report both graphical summaries and Gelman-Rubin diagnostic results.  For the calculation of Gelman-Rubin diagnostics, you will need to install the R package \texttt{coda}. An example of processing chains for calculating Gelman-Rubin diagnostics is given below.

\begin{footnotesize}
\begin{verbatim}
      Processing chains for calculation of Gelman-Rubin diagnostics. Imagine you have 4 chains of
      a multi-parameter problem, and thinning already completed, called par1,par2,par3,par4

      Step one: Converting the chains into mcmc lists.
      library(coda)
      par1<-as.mcmc.list(as.mcmc((par1)))
      par2<-as.mcmc.list(as.mcmc((par2)))
      par3<-as.mcmc.list(as.mcmc((par3)))
      par4<-as.mcmc.list(as.mcmc((par4)))

      Step two: Calculating diagnostics
      
      par.all<-c(par1,par2,par3,par4)
      gelman.diag(par.all)
\end{verbatim}
\end{footnotesize}

d) Fit a standard linear regression,

\[ E(\text{logsf}) = \beta_0 + \beta_1\text{time} \]

to **all the data** using the \texttt{lm} function in R. Report $\hat{\bm \beta}$, and associated 95 \% confidence intervals. Comparing these results to the results from b), do you believe that sequential analysis gave the same results as fitting the regression on the full data.

```{r}
hiron<-read.csv("Hiron.csv")
linear_model_all<-lm(logsf~time, data=hiron)
summary(linear_model_all)
#sum of residuals and calculate the error variance
#SSE<-(1/(length(linear_model_all$residuals)-2))*sum((linear_model_all$residuals)^2)
#xTxinv<-1/((hiron$time)%*%(hiron$time))
confint(linear_model_all)
```
```

e) Report the results of posterior predictive checking requested in b). Do you believe the postulated model was plausible. If not, what do you think is a potential flaw in the postulated model.

```{r}

eff_1<-sum(count1)/5000
eff_2<-sum(count2)/5000
print(eff_1)
print(eff_2)

```

